# -*- coding: utf-8 -*-
"""LinearRegressionTitanic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W_DEWCdI6ln5dinSKYFenCMJNzL-Qlvh

# Step 1: Getting, Preprocessing, Analyze Dataset

import standard libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# run this code cell using shift+enter before moving further.   You will be writing the algorithm from scratch, but these standard packages will make your work cleaner and faster.

import numpy as np # library which more efficiently allows you to work with large multidimensional arrays and matrices.  It has functions that operate on the arrays/matrices
import pandas as pd # built on numpy.  Makes it easier to read in data and clean data among other things
from sklearn.metrics import classification_report
from sklearn.metrics import mean_squared_error, r2_score

from sklearn.preprocessing import StandardScaler # Scaling is suggested when running a gradient descent algorithm
from sklearn.model_selection import train_test_split

import matplotlib # a plotting library
from matplotlib import pyplot as plt
# %matplotlib inline

from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import Lasso

from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import MinMaxScaler

"""import dataset

"""

df = pd.read_csv('titanic_dataset.csv')
indices_to_load = [1, 2,3, 4,5]
X = pd.read_csv('titanic_dataset.csv', usecols=indices_to_load)
X = X.to_numpy()
print(X)

y = df.Survived.to_numpy()
# VERIFY - Print the shape of pclass and target
print('Target: ', y.shape)
print('X: ', X.shape) #all features

"""check for missing values

"""

#how many missing values for each cloumn/feature
missing_values = df.isnull().sum()

# Print the counts of missing values
print(missing_values)

#output zero is because we already cleaned up the dataset before we improt the csv file

"""Preparing the Data"""

# Split the data using train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=42)

#Scaling Data Using Standard Scaler
scale = StandardScaler() #create scaler obj

X_scaled = scale.fit_transform(X)

# Check the shape of X and y vectors.
print("shape of X: ", X_scaled.shape)
print("shape of y: ", y.shape)


#Feature Transformation: Polynomial Features
polynomial = PolynomialFeatures(degree=2)
X_poly = polynomial.fit_transform(X)


#Min/Max Scaling
sca = MinMaxScaler()
X_min_max = sca.fit_transform(X)

#convert y into rank 2 matrix
y_2d = y.reshape(y.shape[0],1)
print("shape of y_2d: ", y_2d.shape)

# Save number of training examples into N and print it
N = 331
print("N: ", N)

"""Next, add a column of ones to the front of $X$:
$$X=\begin{bmatrix}
1 & x^{(1)}_1 & \cdots & x^{(1)}_5\\
\vdots & \vdots & \vdots & \vdots\\
1 & x^{(331)}_1 & \cdots & x^{(331)}_5\\
\end{bmatrix}$$
"""

ones = pd.DataFrame({'Ones': np.ones(N)})
X_1 = np.hstack((X_scaled, ones))
print("shape of X_1: ", X_1.shape) #Shape should be (331,6)

"""Visualize Data"""

#plot relationship between target value(Survived) and each of scaled features

fig, axs = plt.subplots(2, 3, figsize=(10,10))

# top row
axs[0,0].scatter(X_1[:,1],y, s=1)
axs[0,0].set_xlabel(df.columns[1])
axs[0,1].scatter(X_1[:,2],y, s=1)
axs[0,1].set_xlabel(df.columns[2])
axs[0,2].scatter(X_1[:,3],y, s=1)
axs[0,2].set_xlabel(df.columns[3])

#bottom row
axs[1,0].scatter(X_1[:,4],y, s=1)
axs[1,0].set_xlabel(df.columns[4])
axs[1,1].scatter(X_1[:,5],y, s=1)
axs[1,1].set_xlabel(df.columns[5])

for ax in axs.flat:
    ax.set( ylabel='Survived')

fig.tight_layout()

#compute standard correlation coefficient
corr_matrix = np.corrcoef(X.T, y)

# Extract the last row or column (depending on your data structure) for correlation values
correlations = corr_matrix[:-1, -1]  # Assuming y is the last row/column

print(correlations)

"""Visualize How correlated our features are with each other"""

from pandas.plotting import scatter_matrix

X_df = pd.DataFrame(X, columns=['Pclass', 'Sex', 'Age', 'Sibsp', 'Fare'])#convert X numpy to DataFrame
scatter_matrix(X_df[['Pclass', 'Sex', 'Age', 'Sibsp', 'Fare']],figsize=(12, 5))

"""Implement Model w/ Gradient Descent

## Cost Function: Mean Squared Error.
Compute the cost:$$J({\bf w})=\frac{1}{2N}\sum_{i=1}^N(\hat{y}^{(i)}-y^{(i)})^2$$




"""

def compute_cost(X_1, y, w, N):
    cost= np.sum((X_1.dot(w)-y)**2) / (2*N)
    return cost

"""## Gradient Descent"""

def gradient_descent(X_1 , y , learning_rate , w , n , num_iters):
    for i in range(num_iters):
        w = w-learning_rate * (X.T.dot(X.dot(w)-y))/N

        if(i%100==0):
            cost= compute_cost(X,y,w,N)
        print(cost)
    return w
    ###might have potential PROBLEM

"""Batch Gradient Descent

"""

def multiple_linear_reg_model_gda(X_1 , y , n , learning_rate , num_iters):

    #initialize the values of parameter vector w. It should be a column vector of zeros of dimension(d+1,1)
    w = np.zeros((5, 1))

    #calculate the initial cost by calling the function you coded above.
    initial_cost= compute_cost(X_1, y, w, n)
    print("Initial Cost")
    print(initial_cost)

    #calculate the optimized value of gradients by calling the gradient_descent function coded above
    w = gradient_descent(X_1, y, learning_rate, w, n, num_iters)

    # Calculate the cost with the optimized value of w0 and w1 by calling the cost function.
    final_cost = compute_cost(X_1, y, w, n)
    print("Final Cost")
    print(final_cost)

    return w, initial_cost, final_cost

#FIND VALUES OF W
learning_rate = 0.00001
num_iters = 10000
results = multiple_linear_reg_model_gda(X_scaled,y_2d, N, learning_rate, num_iters)

w = results[0]
print("w: ", w, "\ninitial cost: ", results[1], "\nfinal cost: ", results[2])

"""Predict Median

"""

#predict y
def predict(X, w):
    # Ensure X is a 2D array and w is properly sized
    # Insert a bias term at the start of each feature vector in X
    X_aug = np.insert(X, 0, 1, axis=1)  # Add a column of ones for the bias term
    predicted_y = np.dot(w, X_aug)
    return predicted_y
# Call the predict function with a new
x = np.array([8, 41, 6, 1, 320]) # Observe that x is not augmented with a one
# To use our hypothesis, we must first scale x the same way we scaled the training data.

# Before scaling, reshape x to be a 2-dimensional row vector (since our data matrix had each example as a row)
x = x.reshape(1, -1)
# Next, we scale x as we scaled the training data
x_scaled = scale.transform(x)

# Finally, we can predict.
y_predict =  predict(x_scaled, w)
print("The estimated house price is: ", y_predict*100000)

"""Normal Equation Method"""

w_vec = np.linalg.pinv(((X_1).T).dot(X_1)).dot(X_1.T).dot(y_2d)

print("w_vec: ", w_vec)

#Add regularization
# Initialize the Ridge regressor with an alpha value
ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength

# Optionally include scaling in a pipeline
pipeline = make_pipeline(StandardScaler(), ridge_model)

# Fit the model
pipeline.fit(X_train, y_train)

# Predict using the model
y_pred = pipeline.predict(X_test)

# MSE
train_mse = mean_squared_error(y_train, y_tr_pred)
validation_mse = mean_squared_error(y_val, y_val_pred)

# RMSE
train_rmse = np.sqrt(train_mse)
validation_rmse = np.sqrt(validation_mse)

# R^2
train_r2 = r2_score(y_train, y_tr_pred)
validation_r2_val = r2_score(y_val, y_val_pred)

print("Training MSE:" ,train_mse)
print("Validation MSE:", validation_mse)
print("Training RMSE:", train_rmse)
print("Validation RMSE:", validation_rmse)
print("Training R^2:", train_r2)
print("Validation R^2:" ,validation_r2_val)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

model = LinearRegression()
model_lasso = Lasso(alpha=0.1)  # alpha is the regularization strength for Lasso
model_ridge = Ridge(alpha=1.0)

model.fit(X_train, y_train)
model_lasso.fit(X_train, y_train)
model_ridge.fit(X_train, y_train)

y_tr_pred = model.predict(X_train)
y_val_pred = model.predict(X_val)

y_tr_expected_lasso = model_lasso.predict(X_train)
y_validation_expected_lasso = model_lasso.predict(X_val)

y_tr_expected_ridge = model_ridge.predict(X_train)
y_validation_expected_ridge = model_ridge.predict(X_val)


# MSE
train_mse = mean_squared_error(y_train, y_tr_expected_ridge)
validation_mse = mean_squared_error(y_val, y_validation_expected_ridge)

# RMSE
train_rmse = np.sqrt(train_mse)
validation_rmse = np.sqrt(validation_mse)

# R^2
train_r2 = r2_score(y_train, y_tr_expected_ridge)
validation_r2_val = r2_score(y_val, y_validation_expected_ridge)

print("Training MSE:" ,train_mse)
print("Validation MSE:", validation_mse)
print("Training RMSE:", train_rmse)
print("Validation RMSE:", validation_rmse)
print("Training R^2:", train_r2)
print("Validation R^2:" ,validation_r2_val)

#precision recall accuracy F1

#Draw Graph
feature_weights = model.coef_

plt.figure(figsize=(8, 4))
plt.bar(range(len(feature_weights)), feature_weights, color='blue', alpha=0.7)
plt.title('Feature Weights')
plt.xlabel('Features')
plt.ylabel('Weight')

plt.xticks(ticks=range(len(feature_weights)), labels=[f'Feature {i+1}' for i in range(len(feature_weights))])
plt.show()